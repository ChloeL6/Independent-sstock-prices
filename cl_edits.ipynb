{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyspark\n",
    "import os \n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as sf      # sf = spark functions\n",
    "import pyspark.sql.types as st          # st = spark types\n",
    "import datetime as dt\n",
    "\n",
    "# spark = SparkSession.builder.master('local').getOrCreate()\n",
    "\n",
    "# spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+\n",
      "|   Yearmon|CPI|\n",
      "+----------+---+\n",
      "|01-01-1913|9.8|\n",
      "|01-02-1913|9.8|\n",
      "|01-03-1913|9.8|\n",
      "|01-04-1913|9.8|\n",
      "|01-05-1913|9.7|\n",
      "+----------+---+\n",
      "only showing top 5 rows\n",
      "\n",
      "+----+---+---+---+---+---+---+---+---+---+---+---+---+\n",
      "|Year|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec|\n",
      "+----+---+---+---+---+---+---+---+---+---+---+---+---+\n",
      "|1948|3.4|3.8|4.0|3.9|3.5|3.6|3.6|3.9|3.8|3.7|3.8|4.0|\n",
      "|1949|4.3|4.7|5.0|5.3|6.1|6.2|6.7|6.8|6.6|7.9|6.4|6.6|\n",
      "|1950|6.5|6.4|6.3|5.8|5.5|5.4|5.0|4.5|4.4|4.2|4.2|4.3|\n",
      "|1951|3.7|3.4|3.4|3.1|3.0|3.2|3.1|3.1|3.3|3.5|3.5|3.1|\n",
      "|1952|3.2|3.1|2.9|2.9|3.0|3.0|3.2|3.4|3.1|3.0|2.8|2.7|\n",
      "+----+---+---+---+---+---+---+---+---+---+---+---+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_dir = './data'\n",
    "\n",
    "cpi_file = 'US-CPI.csv'\n",
    "unemp_file = 'USUnemployment.csv'\n",
    "\n",
    "c_schema = 'Yearmon string, CPI float'\n",
    "u_schema = 'Year int, Jan float ,Feb float ,Mar float,Apr float ,May float,Jun float ,Jul float ,Aug float ,Sep float,Oct float,Nov float,Dec float'\n",
    "\n",
    "cpi_df = spark.read.csv(os.path.join(data_dir, cpi_file), schema=c_schema, header=True, enforceSchema=True)\n",
    "unemp_df = spark.read.csv(os.path.join(data_dir, unemp_file), schema=u_schema, header=True, enforceSchema=True)\n",
    "\n",
    "cpi_df.show(5)\n",
    "unemp_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------------+\n",
      "|year|  avg_cpi_per_year|\n",
      "+----+------------------+\n",
      "|1953|26.766666571299236|\n",
      "|1957|28.091666380564373|\n",
      "|1987|           113.625|\n",
      "|1956|27.183333079020183|\n",
      "|1936|13.866666634877523|\n",
      "+----+------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+----+---+---+---+---+---+---+---+---+---+---+---+---+-----------------------+\n",
      "|year|jan|feb|mar|apr|may|jun|jul|aug|sep|oct|nov|dec|avg_unemp_rate_per_year|\n",
      "+----+---+---+---+---+---+---+---+---+---+---+---+---+-----------------------+\n",
      "|1948|3.4|3.8|4.0|3.9|3.5|3.6|3.6|3.9|3.8|3.7|3.8|4.0|                   3.75|\n",
      "|1949|4.3|4.7|5.0|5.3|6.1|6.2|6.7|6.8|6.6|7.9|6.4|6.6|                   6.05|\n",
      "|1950|6.5|6.4|6.3|5.8|5.5|5.4|5.0|4.5|4.4|4.2|4.2|4.3|              5.2083335|\n",
      "|1951|3.7|3.4|3.4|3.1|3.0|3.2|3.1|3.1|3.3|3.5|3.5|3.1|              3.2833333|\n",
      "|1952|3.2|3.1|2.9|2.9|3.0|3.0|3.2|3.4|3.1|3.0|2.8|2.7|                  3.025|\n",
      "+----+---+---+---+---+---+---+---+---+---+---+---+---+-----------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col,avg\n",
    "# rename cols in cpi df\n",
    "\n",
    "for column in cpi_df.columns: \n",
    "  cpi_df = cpi_df.withColumnRenamed(column, column.lower())\n",
    "\n",
    "cpi_df = cpi_df.withColumnRenamed('yearmon', 'year')\n",
    "\n",
    "# split year column into month, date, year columns\n",
    "split_cpi_df = cpi_df.withColumn(\"month\", sf.split(sf.col('year'), \"-\").getItem(0))\\\n",
    "              .withColumn('date', sf.split(sf.col('year'), \"-\").getItem(1))\\\n",
    "              .withColumn('year', sf.split(sf.col('year'), \"-\").getItem(2))\n",
    "\n",
    "# cast year col to int for groupby method\n",
    "# cpi_df = cpi_df.withColumn('year', sf.col('year').cast('int'))\n",
    "\n",
    "# rename cols in unemployment df\n",
    "for column in unemp_df.columns: \n",
    "  unemp_df = unemp_df.withColumnRenamed(column, column.lower())\n",
    "\n",
    "# calculate avg unemployment rate per year\n",
    "udf_avg = sf.udf(lambda array: sum(array)/len(array))\n",
    "unemp_df = unemp_df.withColumn(\"avg_unemp_rate_per_year\", udf_avg(sf.array('Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec')).cast('float'))\n",
    "\n",
    "# calculate avg inflation rate per year\n",
    "avg_cpi_df = split_cpi_df.groupBy(sf.col('year')) \\\n",
    "                .agg(sf.avg(sf.col('cpi')).alias('avg_cpi_per_year'))\n",
    "\n",
    "# avg_cpi_df.show(truncate=False)\n",
    "avg_cpi_df.show(5)\n",
    "unemp_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   year  avg_unemp_per_year\n",
      "0  1948            3.750000\n",
      "1  1949            6.050000\n",
      "2  1950            5.208333\n",
      "3  1951            3.283333\n",
      "4  1952            3.025000\n",
      "year                    int64\n",
      "avg_unemp_per_year    float64\n",
      "dtype: object\n",
      "   year        cpi\n",
      "0  1913   9.883333\n",
      "1  1914  10.016667\n",
      "2  1915  10.108333\n",
      "3  1916  10.883333\n",
      "4  1917  12.825000\n",
      "5  1918  15.041667\n",
      "6  1919  17.333333\n",
      "7  1920  20.041667\n",
      "8  1921  17.850000\n",
      "9  1922  16.750000\n",
      "year      int64\n",
      "cpi     float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "data_dir = 'dsa-airflow/data'\n",
    "\n",
    "cpi_file = 'US-CPI.csv'\n",
    "unemp_file = 'USUnemployment.csv'\n",
    "\n",
    "# read and rename cpi file / parse_dates=['Yearmon']\n",
    "cpi  = pd.read_csv(os.path.join(data_dir, cpi_file), header=0)\n",
    "cpi = cpi.rename(columns={'Yearmon': 'year', 'CPI': 'cpi'})\n",
    "cpi[['month', 'date', 'year']] = cpi.year.str.split(\"-\", expand=True)\n",
    "\n",
    "# read and rename unemployment file\n",
    "unemp = pd.read_csv(os.path.join(data_dir, unemp_file), header=0)\n",
    "unemp.columns = unemp.columns.str.lower()\n",
    "\n",
    "\n",
    "# calculate the avg yearly unemp. rate\n",
    "columns = ['jan',\t'feb',\t'mar',\t'apr',\t'may'\t,'jun',\t'jul',\t'aug',\t'sep',\t'oct',\t'nov',\t'dec']\n",
    "unemp['avg_unemp_per_year']  = unemp[columns].mean(axis=1)\n",
    "avg_unemp_per_year  = unemp[['year', 'avg_unemp_per_year']]\n",
    "\n",
    "# calculate the avg yearly cpi rate\n",
    "avg_cpi_per_year = cpi.groupby('year').mean('cpi')\n",
    "avg_cpi_per_year = avg_cpi_per_year.reset_index()\n",
    "avg_cpi_per_year['year'] = avg_cpi_per_year['year'].astype('int')\n",
    "\n",
    "print(avg_unemp_per_year.head(5))\n",
    "print(avg_unemp_per_year.dtypes)\n",
    "print(avg_cpi_per_year.head(10))\n",
    "print(avg_cpi_per_year.dtypes)\n",
    "# unemp.dtypes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created a BiqQuery client\n",
      "Project: team-week-3\n",
      "Created dataset team-week-3.inflation_unemployment\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n",
    "\n",
    "# create full table id\n",
    "PROJECT_ID = \"team-week-3\"\n",
    "DATASET_ID = \"inflation_unemployment\"\n",
    "\n",
    "# create bigquery client\n",
    "key_path = os.path.expanduser(\"/home/chloe_ycl/.creds/team-week-3.json\")\n",
    "credentials = service_account.Credentials.from_service_account_file(key_path, \n",
    "                                                              scopes=[\"https://www.googleapis.com/auth/cloud-platform\"])\n",
    "\n",
    "client = bigquery.Client(credentials=credentials, project=credentials.project_id)\n",
    "\n",
    "print(\"Successfully created a BiqQuery client\")\n",
    "print(f\"Project: {client.project}\")\n",
    "\n",
    "# create dataset\n",
    "dataset_id = \"{}.inflation_unemployment\".format(client.project)\n",
    "\n",
    "#construct dataset obj to send to API\n",
    "dataset = bigquery.Dataset(dataset_id)\n",
    "\n",
    "#specify the location\n",
    "dataset.location = \"US\"\n",
    "\n",
    "dataset = client.create_dataset(dataset, timeout=30, exists_ok=True) #Make an API request\n",
    "print(\"Created dataset {}.{}\".format(client.project, dataset.dataset_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funtion to load table from dataframes\n",
    "\n",
    "def load_table(\n",
    "    df: pd.DataFrame, \n",
    "    client: bigquery.Client, \n",
    "    table_name: str, \n",
    "    schema: bigquery.SchemaField,\n",
    "    create_disposition: str = 'CREATE_IF_NEEDED', \n",
    "    write_disposition: str = 'WRITE_TRUNCATE'\n",
    "    ) -> None:\n",
    "    \n",
    "    job_config = bigquery.LoadJobConfig(\n",
    "        create_disposition=create_disposition,\n",
    "        write_disposition=write_disposition,\n",
    "        schema=schema\n",
    "    )\n",
    "    \n",
    "    job = client.load_table_from_dataframe(df, destination=table_name, job_config=job_config)\n",
    "    job.result()        # wait for the job to finish\n",
    "\n",
    "\n",
    "\n",
    "# create our own bigquery schema\n",
    "CPI_METADATA = {\n",
    "    'cpi_rates': {\n",
    "        'table_name': 'cpi_rates',\n",
    "        'schema' : [\n",
    "            bigquery.SchemaField(\"year\", \"INTEGER\", mode=\"REQUIRED\"),\n",
    "            bigquery.SchemaField(\"cpi\", \"FLOAT\", mode=\"REQUIRED\")]\n",
    "    }}\n",
    "\n",
    "UNEMP_METADATA = {\n",
    "    'unemployment_rates' :{\n",
    "        'table_name': 'unemployment_rates',\n",
    "        'schema': [\n",
    "            bigquery.SchemaField(\"year\", \"INTEGER\", mode=\"REQUIRED\"),\n",
    "            bigquery.SchemaField(\"avg_unemp_per_year\", \"FLOAT\", mode=\"REQUIRED\")]\n",
    "    }}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load to bq\n",
    "table_name = f\"{PROJECT_ID}.{DATASET_ID}.{CPI_METADATA['cpi_rates']['table_name']}\"\n",
    "schema = CPI_METADATA['cpi_rates']['schema']\n",
    "load_table(avg_cpi_per_year, client, table_name, schema)\n",
    "\n",
    "table_name = f\"{PROJECT_ID}.{DATASET_ID}.{UNEMP_METADATA['unemployment_rates']['table_name']}\"\n",
    "schema = UNEMP_METADATA['unemployment_rates']['schema']\n",
    "load_table(avg_unemp_per_year, client, table_name, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query Stock file from bq\n",
    "STOCK_DATASET_ID = 'tech_stocks_world_events'\n",
    "\n",
    "stocks = f\"\"\" SELECT * FROM {PROJECT_ID}.{STOCK_DATASET_ID}.stocks \"\"\"\n",
    "\n",
    "result = client.query(stocks)\n",
    "\n",
    "# read to dataframe\n",
    "df = result.to_dataframe()\n",
    "\n",
    "# read to csv file\n",
    "df.to_csv(os.path.join(data_dir, 'stocks.csv'), header=True)\n",
    "\n",
    "# print(stocks_df.dtypes)\n",
    "# stocks_df\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "13cd2dfbaa0c360159abf2d98dc2f30be3ac863f44bedcc9d13de3e4ef3ae6ec"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
